# Autoencoder
<img src="Poster.jpg"
     alt="Markdown poster"
     style="float: left; padding-right:10px;padding-bottom:10px; margin-right: 10px;" />
<p style="clear:left;"></p>
     
## Our Nural Network Project :
<pre>
                                  Autoencoder which is an artificial neural network used for unsupervised learning of
                                  efficient codings. The aim of an autoencoder is to learn a representation (encoding) 
                                  for a set of data, typically for the purpose of dimensionality reduction.
</pre>

## Unsupervised Learning :
<pre>
                                  Unsupervised machine learning is the machine learning task 
                                  of inferring a function to describe hidden structure from 
                                  "unlabeled" data (a classification or categorization is not 
                                  included in the observations).

</pre>

## Architecture :
<pre>
                                  The nural network is built of 4 layers and an input layer (3 hidden layers) 
                                  of sizes 784, 392, 196, 392 and 784 respectively.

</pre>

### This project's Dataset is in <a href="https://github.com/Galileo103/Autoencoder/tree/master/MNIST_data" style="text-decoration:none"> MNIST_data </a> folder

### There are some snapshots for the output of the program with defferent settings in <a href="https://github.com/Galileo103/Autoencoder/tree/master/Snapshot" style="text-decoration:none"> Snapshots </a>


## The References  <a href="https://github.com/Galileo103/Autoencoder/tree/master/References" style="text-decoration:none"> here </a>

<ol>
  <li><font size="4">Autoencoders, Unsupervised Learning, and Deep Architectures</font></li> is the paper from University of California, Irvine
  <li><font size="4">Andrew Ng Lecture about Autoencoders</font></li> is the stanford presentation for professor Andrew that takes about autoencoder
</ol>
